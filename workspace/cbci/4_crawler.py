"""
4_crawler.py

âœ… ëª©í‘œ:
- ë‚ ì§œ(days_back) ì—†ì´ ìë™ ì¦ë¶„ í¬ë¡¤ë§ (ì—°í•©ë‰´ìŠ¤ API)
- ë§ˆì§€ë§‰ í¬ë¡¤ë§ ë‚ ì§œ +1ì¼ ~ ì˜¤ëŠ˜ê¹Œì§€ ìˆ˜ì§‘
- ì‚½ì… ì „ ì¤‘ë³µ ì œê±°:
  1) CID ê¸°ì¤€ (ê¸°ì—…(GROUP) ë‹¨ìœ„)
  2) ì œëª© ìœ ì‚¬/ì¤‘ë³µ (ê¸°ì—…(GROUP) ë‹¨ìœ„)
- ê°ì„± ë¶„ì„ í†µí•©: ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì‹œ AI ê°ì„± ë¶„ì„ ë° í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ ì ìš©

âœ… ì£¼ìš” ê¸°ëŠ¥:
1) ìë™ ì¦ë¶„ í¬ë¡¤ë§: MongoDB ë° íŒŒì¼ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ì—†ì´ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
2) ê°ì„± ë¶„ì„ ë°±í•„: ê¸°ì¡´ì— ì €ì¥ëœ ê¸°ì‚¬ë“¤ì— ëŒ€í•œ ì‚¬í›„ ê°ì„± ë¶„ì„ ê¸°ëŠ¥
3) ì¤‘ë³µ ì •ë¦¬: MongoDB ë‚´ì˜ ê¸°ì¡´ ì¤‘ë³µ ë°ì´í„°(CID/ì œëª©) íƒì§€ ë° ì‚­ì œ

ì‚¬ìš© ì˜ˆ)
1) ì¼ë°˜ í¬ë¡¤ë§ (ë§¤ì¼ ìë™ ì¦ë¶„ + ê°ì„± ë¶„ì„ í¬í•¨):
   python 4_crawler.py

2) ê°ì„± ë¶„ì„ ë°±í•„ (ë¯¸ë¶„ì„ ê¸°ì‚¬ ëŒ€ìƒ):
   python 4_crawler.py --backfill-sentiment --backfill-limit 500

3) ê°ì„± ë¶„ì„ ê°•ì œ ì¬ë¶„ì„ (ê¸°ì¡´ ê²°ê³¼ ë®ì–´ì“°ê¸°):
   python 4_crawler.py --backfill-sentiment --backfill-limit 500 --force

4) ê¸°ì¡´ ì¤‘ë³µ ë°ì´í„° ì •ë¦¬ (DRY RUN):
   python 4_crawler.py --cleanup-existing

5) ê¸°ì¡´ ì¤‘ë³µ ë°ì´í„° ì‹¤ì œ ì‚­ì œ ì ìš©:
   python 4_crawler.py --cleanup-existing --apply
"""

import requests
import csv
import json
import re
import time
import os
import sys
import argparse
import difflib
from dataclasses import dataclass, field
from datetime import datetime, timedelta, time as dtime, date as ddate
from typing import Optional, Dict, Tuple, List, Any
from urllib.parse import urljoin
from urllib3.util import create_urllib3_context
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from zoneinfo import ZoneInfo

load_dotenv()

# =========================
# âœ… ìµœì´ˆ ì‹¤í–‰ ì‹œ ìë™ ìˆ˜ì§‘ ë²”ìœ„(ì‚¬ìš©ì ì„¤ì • ë¶ˆí•„ìš”)
# =========================
BOOTSTRAP_DAYS = 120  # stateë„ Mongoë„ ì—†ìœ¼ë©´ ìµœê·¼ 120ì¼ë¶€í„° ì²« ìˆ˜ì§‘

try:
    from pymongo import MongoClient, UpdateOne
    from bson import ObjectId
except Exception:
    MongoClient = None
    UpdateOne = None
    ObjectId = None

KST = ZoneInfo("Asia/Seoul")

# ê°ì„± ë¶„ì„ ì„œë¹„ìŠ¤(ë¡œì»¬, ë¬´ë¹„ìš©)
try:
    from sentiment_service import service as sentiment_service
except Exception:
    sentiment_service = None


# =========================
# âœ… ì„¤ì •(ë‚ ì§œ ê´€ë ¨ ì„¤ì • ì—†ìŒ)
# =========================
@dataclass
class CrawlConfig:
    # ì†ë„/í˜ì´ì§•
    page_size: int = 15
    timeout: int = 10
    sleep_sec: float = 0.7
    max_pages: int = 20

    # ì—”ë“œí¬ì¸íŠ¸
    base_url: str = "https://ars.yna.co.kr/api/v2/search.basic"
    results_key: str = "YIB_KR_A"

    # í•„í„°
    cattr: str = "A"
    div_code: str = "01,02,05,11"
    scope: str = "all"
    sort: str = "date"
    channel: str = "basic_kr"

    # ì €ì¥
    output_prefix: str = "yna_news"

    # ë””ë²„ê·¸
    debug_total: bool = True
    debug_top_keys: bool = False

    # ë³¸ë¬¸/ì´ë¯¸ì§€ í¬ë¡¤ë§
    fetch_content: bool = True
    fetch_image: bool = True
    content_timeout: int = 12
    content_sleep_sec: float = 0.35
    content_max_chars: int = 30000
    content_retries: int = 2
    content_retry_backoff: float = 1.3

    # MongoDB ì—…ë¡œë“œ
    upload_to_mongo: bool = True
    mongo_uri: str = os.getenv("MONGO_URI", "")
    mongo_db: str = os.getenv("MONGO_DB", "news")
    mongo_collection: str = os.getenv("MONGO_COL", "yna_news")
    mongo_upsert: bool = True
    mongo_batch_size: int = 500
    mongo_ensure_unique_cid: bool = False  # ì¤‘ë³µ ì •ë¦¬ í›„ True ê¶Œì¥

    # âœ… ê°ì„± ë¶„ì„ ì„¤ì •
    sentiment_enabled: bool = True
    sentiment_filter: List[str] = field(default_factory=list) # ì˜ˆ: ['positive', 'negative']ë§Œ ìœ ì§€

    # ë§ˆì§€ë§‰ í¬ë¡¤ë§ ë‚ ì§œ(state) ì €ì¥ ê²½ë¡œ
    state_path: str = "crawler_state.json"

    # âœ… (ì‚½ì… ì „) CID/ì œëª© ì¤‘ë³µ ì œê±° ì˜µì…˜
    pre_insert_dedup_enable: bool = True

    # 1ì°¨ CID dedup: DBì—ë„ ê°™ì€ CIDê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ(ê¶Œì¥)
    pre_insert_skip_if_cid_exists_in_db: bool = True

    # 2ì°¨ ì œëª© ìœ ì‚¬/ì¤‘ë³µ í•„í„°
    title_dedup_enable: bool = True
    title_similarity_threshold: float = 0.93   # difflib ratio
    title_jaccard_threshold: float = 0.88      # token jaccard
    title_dedup_db_lookup: bool = True         # DB ìµœê·¼ ê¸°ì‚¬ì™€ë„ ë¹„êµ
    title_dedup_db_limit_per_group: int = 3000 # ê·¸ë£¹ë³„ ìµœê·¼ Nê°œ ì œëª© ë¡œë“œ
    title_dedup_compare_recent_keep: int = 300 # ë°°ì¹˜ ë‚´ ìµœê·¼ ìœ ì§€ ì œëª© ë¹„êµ ê°œìˆ˜(ì†ë„)
    title_dedup_only_within_days: int = 3      # ë„ˆë¬´ ì˜¤ë˜ëœ ê¸°ì‚¬ì™€ëŠ” ë¹„êµ ì•ˆ í•¨(ì˜¤íƒ ë°©ì§€)
    title_dedup_verbose: bool = True

    # ì¶”ê°€ íŒŒë¼ë¯¸í„° ì£¼ì…(í•„ìš” ì‹œ)
    extra_params: dict = field(default_factory=dict)


CONFIG = CrawlConfig(
    div_code="01,02,05,11",
    cattr="A",
    fetch_content=True,
    fetch_image=True,
    upload_to_mongo=True,
    debug_total=True,
    max_pages=20,
    mongo_ensure_unique_cid=False,
    state_path="crawler_state.json",
)


# =========================
# âœ… state: ë§ˆì§€ë§‰ í¬ë¡¤ë§ ë‚ ì§œ ì €ì¥/ë¡œë“œ
# =========================
def load_last_crawled_date(path: str) -> Optional[ddate]:
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            obj = json.load(f)
        s = (obj.get("last_crawled_date_kst") or "").strip()
        if not s:
            return None
        return datetime.strptime(s, "%Y-%m-%d").date()
    except Exception:
        return None


def save_last_crawled_date(path: str, d: ddate) -> None:
    with open(path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "last_crawled_date_kst": d.strftime("%Y-%m-%d"),
                "saved_at_kst": datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S"),
            },
            f,
            ensure_ascii=False,
            indent=2,
        )


# =========================
# âœ… MongoDBì—ì„œ ë§ˆì§€ë§‰ "ì‹œê°"(datetime) ìë™ ê°ì§€
# =========================
def detect_last_dt_from_mongo(config: CrawlConfig) -> Optional[datetime]:
    """ë¬¸ì„œì˜ DATETIME í•„ë“œì—ì„œ ê°€ì¥ ìµœì‹  ì‹œê°ì„ datetimeìœ¼ë¡œ ë°˜í™˜.
    - ìš°ì„  BSON date íƒ€ì…ì„ í™•ì¸(ì •í™•í•œ ì‹œê°„)
    - ë‹¤ìŒ ë¬¸ìì—´ íƒ€ì…(YYYY-MM-DD HH:MM:SS) íŒŒì‹±
    - ë‘˜ ë‹¤ ì—†ìœ¼ë©´ None
    """
    if not config.upload_to_mongo or not MongoClient or not config.mongo_uri:
        return None
    client = None
    try:
        client = MongoClient(config.mongo_uri, serverSelectionTimeoutMS=4000)
        col = client[config.mongo_db][config.mongo_collection]

        # 1) DATETIME(date) ìµœì‹ 
        doc = list(
            col.find({"DATETIME": {"$type": "date"}}, {"DATETIME": 1})
            .sort("DATETIME", -1)
            .limit(1)
        )
        if doc:
            dtv = doc[0].get("DATETIME")
            if isinstance(dtv, datetime):
                # BSON datetimeì€ UTCì¼ ìˆ˜ ìˆìœ¼ë‚˜, ë³¸ ì½”ë“œëŠ” naiveë¡œ ì·¨ê¸‰
                return dtv.replace(tzinfo=None)

        # 2) DATETIME(string) ìµœì‹  (í˜•ì‹ì´ YYYY-MM-DD HH:MM:SSë©´ ì‹œê°„ê¹Œì§€ ë°˜ì˜)
        doc2 = list(
            col.find({"DATETIME": {"$type": "string"}}, {"DATETIME": 1})
            .sort("DATETIME", -1)
            .limit(1)
        )
        if doc2:
            s = (doc2[0].get("DATETIME") or "").strip()
            if s:
                try:
                    return datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
                except Exception:
                    pass

        return None
    except Exception:
        return None
    finally:
        try:
            if client:
                client.close()
        except Exception:
            pass


# =========================
# âœ… MongoDBì—ì„œ ë§ˆì§€ë§‰ ë‚ ì§œ ìë™ ê°ì§€(í•˜ìœ„ í˜¸í™˜: date ë‹¨ìœ„)
# =========================
def detect_last_date_from_mongo(config: CrawlConfig) -> Optional[ddate]:
    if not config.upload_to_mongo or not MongoClient or not config.mongo_uri:
        return None
    client = None
    try:
        client = MongoClient(config.mongo_uri, serverSelectionTimeoutMS=4000)
        col = client[config.mongo_db][config.mongo_collection]

        # DATETIME(date) ìµœì‹ 
        doc = list(
            col.find({"DATETIME": {"$type": "date"}}, {"DATETIME": 1})
            .sort("DATETIME", -1)
            .limit(1)
        )
        if doc:
            dtv = doc[0].get("DATETIME")
            if isinstance(dtv, datetime):
                return dtv.date()

        # DATETIME(string) ìµœì‹  (í˜•ì‹ì´ YYYY-MM-DD HH:MM:SSë©´ ë¬¸ìì—´ ì •ë ¬ OK)
        doc2 = list(
            col.find({"DATETIME": {"$type": "string"}}, {"DATETIME": 1})
            .sort("DATETIME", -1)
            .limit(1)
        )
        if doc2:
            s = (doc2[0].get("DATETIME") or "").strip()
            if s:
                try:
                    return datetime.strptime(s, "%Y-%m-%d %H:%M:%S").date()
                except Exception:
                    pass

        return None
    except Exception:
        return None
    finally:
        try:
            if client:
                client.close()
        except Exception:
            pass


# =========================
# âœ… ì´ë²ˆ ì‹¤í–‰ì˜ since_dt ìë™ ê³„ì‚° (ì‹œê°„ ë‹¨ìœ„, ê²¹ì¹¨ ë³´ì¥)
# =========================
def compute_since_dt_auto(config: CrawlConfig) -> Optional[datetime]:
    # 1) Mongo ìµœì‹  ì‹œê° ìš°ì„ 
    last_dt = detect_last_dt_from_mongo(config)
    source = "mongo-dt" if last_dt else ""

    # 2) í•˜ìœ„í˜¸í™˜: ë‚ ì§œ(state/mongo-date) ê¸°ë°˜
    if last_dt is None:
        last_date = load_last_crawled_date(config.state_path)
        src2 = "state" if last_date else ""
        if last_date is None:
            last_date = detect_last_date_from_mongo(config)
            src2 = "mongo-date" if last_date else "bootstrap"
        if last_date is None:
            start_dt = (datetime.now(KST).replace(tzinfo=None) - timedelta(days=BOOTSTRAP_DAYS))
            print(f"ğŸŸ¡ state/mongo ì—†ìŒ â†’ ìë™ ë¶€íŠ¸ìŠ¤íŠ¸ë©: ìµœê·¼ {BOOTSTRAP_DAYS}ì¼ ìˆ˜ì§‘ (since={start_dt})")
            return start_dt
        # ë‚ ì§œë§Œ ìˆì„ ë•ŒëŠ” ìì •ë¶€í„° ì‹œì‘(ê³¼ê±° ë¡œì§ í˜¸í™˜)
        last_dt = datetime.combine(last_date, dtime.max).replace(microsecond=0)
        source = src2

    # 3) ê²¹ì¹¨(Overlap) ì ìš©: ìµœê·¼ ìˆ˜ì§‘ ì‹œê°ì—ì„œ ì¼ì • ì‹œê°„ ì´ì „ìœ¼ë¡œ ë‹¹ê²¨ ìˆ˜ì§‘
    OVERLAP_MINUTES = 120  # 2ì‹œê°„ ê²¹ì¹¨ìœ¼ë¡œ ëˆ„ë½ ë°©ì§€
    since_dt = (last_dt - timedelta(minutes=OVERLAP_MINUTES))
    print(f"ğŸŸ¢ ìë™ ì¦ë¶„ ê¸°ì¤€({source}): last_dt={last_dt} â†’ overlap={OVERLAP_MINUTES}m â†’ since={since_dt}")
    return since_dt


# =========================
# SSL ì´ìŠˆ í•´ê²° ì–´ëŒ‘í„°
# =========================
class LegacySSLAdapter(requests.adapters.HTTPAdapter):
    def init_poolmanager(self, *args, **kwargs):
        ctx = create_urllib3_context()
        ctx.load_default_certs()
        ctx.options |= 0x4  # ssl.OP_LEGACY_SERVER_CONNECT
        kwargs["ssl_context"] = ctx
        return super().init_poolmanager(*args, **kwargs)


# =========================
# UA/í—¤ë”
# =========================
def safe_user_agent():
    try:
        ua = UserAgent()
        return ua.random
    except Exception:
        return (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )


def headers_for_api():
    return {
        "User-Agent": safe_user_agent(),
        "Referer": "https://www.yna.co.kr/",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
        "Connection": "keep-alive",
    }


def headers_for_html():
    return {
        "User-Agent": safe_user_agent(),
        "Referer": "https://www.yna.co.kr/",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Connection": "keep-alive",
    }


# =========================
# ìœ í‹¸(ì¿¼ë¦¬/ì •ê·œí™”)
# =========================
def build_queries_company_plus_person(company_kw: str, person_kw: str) -> List[str]:
    company_kw = (company_kw or "").strip()
    person_kw = (person_kw or "").strip()
    if company_kw and person_kw:
        return [f"{company_kw} {person_kw}"]
    return []


def normalize_text(text: str) -> str:
    if not text:
        return ""
    text = text.replace("\xa0", " ")
    text = re.sub(r"[ \t]+", " ", text)
    lines = [ln.strip() for ln in text.splitlines()]
    lines = [ln for ln in lines if ln]
    return "\n".join(lines).strip()


# =========================
# âœ… ì œëª© ì¤‘ë³µ(ìœ ì‚¬ë„)ìš© ì •ê·œí™”
# =========================
_TITLE_TAGS_RE = re.compile(r"(\[.*?\]|\(.*?\))")
_TITLE_PUNCT_RE = re.compile(r"[^\wê°€-í£\s]")
_TITLE_WS_RE = re.compile(r"\s+")
_COMMON_NOISE = [
    "ì†ë³´", "ì¢…í•©", "ë‹¨ë…", "ì‚¬ì§„", "ì˜ìƒ", "ê·¸ë˜í”½", "ì¸í„°ë·°", "ë¥´í¬",
    "ì¬ì†¡ê³ ", "ìˆ˜ì •", "ì •ì •", "ì¶”ê°€", "ì—…ë°ì´íŠ¸"
]

def title_key(title: str) -> str:
    if not title:
        return ""
    t = title.strip()
    # ê´„í˜¸/ëŒ€ê´„í˜¸ íƒœê·¸ ì œê±°
    t = _TITLE_TAGS_RE.sub(" ", t)
    # í”í•œ ë…¸ì´ì¦ˆ ë‹¨ì–´ ì œê±°(ë„ˆë¬´ ê³µê²©ì ì´ë©´ ì—¬ê¸° ì¤„ì´ë©´ ë¨)
    for w in _COMMON_NOISE:
        t = re.sub(rf"\b{re.escape(w)}\b", " ", t, flags=re.IGNORECASE)
    t = t.lower()
    t = _TITLE_PUNCT_RE.sub(" ", t)
    t = _TITLE_WS_RE.sub(" ", t).strip()
    return t

def tokenize_title(tkey: str) -> List[str]:
    if not tkey:
        return []
    toks = [x for x in tkey.split() if x]
    # 1ê¸€ì í† í°ì€ ë…¸ì´ì¦ˆê°€ ë§ì•„ì„œ ì œê±°
    toks = [x for x in toks if len(x) >= 2]
    return toks

def jaccard(a: List[str], b: List[str]) -> float:
    sa, sb = set(a), set(b)
    if not sa and not sb:
        return 1.0
    if not sa or not sb:
        return 0.0
    return len(sa & sb) / len(sa | sb)

def sim_ratio(a: str, b: str) -> float:
    # difflibì€ ê¸¸ì´ê°€ ê¸¸ì–´ë„ ê½¤ ì•ˆì •ì 
    return difflib.SequenceMatcher(None, a, b).ratio()

def is_title_duplicate(a_title: str, b_title: str, cfg: CrawlConfig) -> bool:
    ak = title_key(a_title)
    bk = title_key(b_title)
    if not ak or not bk:
        return False
    if ak == bk:
        return True
    r = sim_ratio(ak, bk)
    if r >= cfg.title_similarity_threshold:
        return True
    ja = jaccard(tokenize_title(ak), tokenize_title(bk))
    if ja >= cfg.title_jaccard_threshold:
        return True
    return False


# =========================
# âœ… ë³¸ë¬¸ ì •ë¦¬(ì—°í•©ë‰´ìŠ¤ UI ì œê±°)
# =========================
_UI_TOKENS = ("êµ¬ë…", "êµ¬ë…ì¤‘", "ì´ì „", "ë‹¤ìŒ", "ì´ë¯¸ì§€ í™•ëŒ€", "ì´ë¯¸ì§€í™•ëŒ€")
_UI_LINE_RE = re.compile(r"^\s*(êµ¬ë…|êµ¬ë…ì¤‘|ì´ì „|ë‹¤ìŒ|ì´ë¯¸ì§€\s*í™•ëŒ€|ì´ë¯¸ì§€í™•ëŒ€)\s*$")
_EMAIL_RE = re.compile(r"\b[A-Za-z0-9._%+-]+@yna\.co\.kr\b", re.IGNORECASE)
_OKJEBO_PHRASE_RE = re.compile(r"ì œë³´ëŠ”\s*ì¹´ì¹´ì˜¤í†¡\s*okjebo", re.IGNORECASE)
_COPYRIGHT_RE = re.compile(
    r"(ì €ì‘ê¶Œì\s*\(c\)\s*ì—°í•©ë‰´ìŠ¤|ë¬´ë‹¨\s*ì „ì¬|ì¬ë°°í¬\s*ê¸ˆì§€|ì—°í•©ë‰´ìŠ¤\s*ë¬´ë‹¨|â“’\s*ì—°í•©ë‰´ìŠ¤)",
    re.IGNORECASE,
)
_RELATED_HEADER_RE = re.compile(r"^\s*ê´€ë ¨\s*ë‰´ìŠ¤\s*$|^\s*ê´€ë ¨ë‰´ìŠ¤\s*$|^\s*ê´€ë ¨\s*ê¸°ì‚¬\s*$|^\s*ê´€ë ¨ê¸°ì‚¬\s*$")

def pre_split_glued_ui_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r"(ê´€ë ¨\s*ë‰´ìŠ¤|ê´€ë ¨ë‰´ìŠ¤|ê´€ë ¨\s*ê¸°ì‚¬|ê´€ë ¨ê¸°ì‚¬)", r"\n\1\n", text)
    text = re.sub(r"(ì´ì „)\s*(ë‹¤ìŒ)", r"\1\n\2", text)
    text = re.sub(r"(ë‹¤ìŒ)(?=[ê°€-í£])", r"\1\n", text)
    text = re.sub(r"(ë‹¤ìŒ)(?=[A-Za-z0-9._%+-]+@)", r"\1\n", text)
    text = re.sub(r"(ì´ë¯¸ì§€\s*í™•ëŒ€)(?=[ê°€-í£])", r"\1\n", text)
    text = re.sub(r"(ì´ë¯¸ì§€í™•ëŒ€)(?=[ê°€-í£])", r"\1\n", text)
    text = re.sub(r"(ë‹¤ìŒ)\s*(<ì €ì‘ê¶Œì)", r"\1\n\2", text)
    return text

def remove_reporter_ui_blocks_anywhere(lines: List[str]) -> List[str]:
    out = []
    i = 0
    n = len(lines)

    def is_name_like(s: str) -> bool:
        s = s.strip()
        if not (2 <= len(s) <= 10):
            return False
        return bool(re.fullmatch(r"[ê°€-í£Â·\s]+", s))

    while i < n:
        cur = lines[i].strip()

        if i + 1 < n and lines[i + 1].strip() == "ê¸°ì" and is_name_like(cur):
            i += 2
            while i < n:
                t = lines[i].strip()
                if _UI_LINE_RE.match(t):
                    i += 1
                    continue
                if any(tok in t for tok in _UI_TOKENS) and len(t) <= 24:
                    i += 1
                    continue
                break
            continue

        if re.fullmatch(r"[ê°€-í£Â·\s]{2,10}\s*ê¸°ì", cur):
            i += 1
            while i < n and _UI_LINE_RE.match(lines[i].strip()):
                i += 1
            continue

        out.append(lines[i])
        i += 1

    return out

def is_headline_like(line: str) -> bool:
    s = line.strip()
    if not s:
        return False
    if len(s) < 8 or len(s) > 140:
        return False
    if _UI_LINE_RE.match(s) or s == "ê¸°ì":
        return False
    if _OKJEBO_PHRASE_RE.search(s) or _COPYRIGHT_RE.search(s):
        return False
    if any(ch in s for ch in ["â€¦", "\"", "â€œ", "â€", "(", ")", "Â·", "â€”", "-"]):
        return True
    if re.search(r"\d", s):
        return True
    if s.endswith(".") or s.endswith("ë‹¤.") or s.endswith("ë‹¤"):
        return False
    return False

def remove_related_news_blocks(lines: List[str]) -> List[str]:
    out = []
    i = 0
    n = len(lines)

    while i < n:
        ln = lines[i].strip()
        if _RELATED_HEADER_RE.match(ln):
            i += 1
            while i < n and is_headline_like(lines[i]):
                i += 1
            continue
        out.append(lines[i])
        i += 1

    return out

def clean_yna_content(text: str) -> str:
    if not text:
        return ""

    text = pre_split_glued_ui_text(text)
    text = _OKJEBO_PHRASE_RE.sub("", text)
    text = _EMAIL_RE.sub("", text)
    text = re.sub(r"\(\s*\)", "", text)

    lines = [ln.strip() for ln in text.splitlines()]
    lines = [ln for ln in lines if ln]

    lines = [ln for ln in lines if not _UI_LINE_RE.match(ln)]
    lines = remove_related_news_blocks(lines)
    lines = remove_reporter_ui_blocks_anywhere(lines)
    lines = [ln for ln in lines if not _OKJEBO_PHRASE_RE.search(ln)]

    cut_idx = None
    for idx, ln in enumerate(lines):
        if _COPYRIGHT_RE.search(ln):
            cut_idx = idx
            break
        if ln.startswith("<ì €ì‘ê¶Œì") or ln.startswith("ï¼œì €ì‘ê¶Œì"):
            cut_idx = idx
            break
    if cut_idx is not None:
        lines = lines[:cut_idx]

    cleaned = "\n".join(lines).strip()
    cleaned = re.sub(r"[ \t]+", " ", cleaned).strip()
    return cleaned


# =========================
# âœ… ì´ë¯¸ì§€ URL ì •ë¦¬
# =========================
_IMG_HOST_PATH_RE = re.compile(r"^/img\d+\.yna\.co\.kr/")

def absolutize_img_url(src: str, base_url: str) -> str:
    if not src:
        return ""
    src = src.strip()
    if not src or src.startswith("data:"):
        return ""

    if src.startswith("http://") or src.startswith("https://"):
        return src
    if src.startswith("//"):
        return "https:" + src
    if _IMG_HOST_PATH_RE.match(src):
        return "https://" + src.lstrip("/")
    return urljoin(base_url, src)

def uniq_keep_order(items: List[str]) -> List[str]:
    seen = set()
    out = []
    for x in items:
        if not x:
            continue
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


# =========================
# ëª©ë¡ API í˜¸ì¶œ(í˜ì´ì§• ì§€ì›)
# =========================
def fetch_articles_paged(session: requests.Session, config: CrawlConfig, query: str, since_dt: datetime):
    collected = []
    last_raw = None

    for page_no in range(1, config.max_pages + 1):
        params = {
            "query": query,
            "page_no": page_no,
            "page_size": config.page_size,
            "scope": config.scope,
            "sort": config.sort,
            "channel": config.channel,
            "div_code": config.div_code,
            "cattr": config.cattr,
        }
        if config.extra_params:
            params.update(config.extra_params)

        resp = session.get(config.base_url, params=params, headers=headers_for_api(), timeout=config.timeout)
        resp.raise_for_status()
        data = resp.json()
        last_raw = data

        if config.debug_top_keys and page_no == 1:
            print("DEBUG top keys:", list(data.keys())[:10])

        if config.results_key not in data:
            break

        if config.debug_total and page_no == 1:
            total = data.get(config.results_key, {}).get("total")
            print(f"DEBUG total({query}):", total)

        results = data[config.results_key].get("result", []) or []
        if not results:
            break

        stop_paging = False
        for art in results:
            cid = art.get("CID")
            dt_str = art.get("DATETIME")
            title = art.get("TITLE") or ""

            if not cid or not dt_str:
                continue

            try:
                art_dt = datetime.strptime(dt_str, "%Y%m%d%H%M%S")  # KST ë¡œì»¬ë¡œ ê°„ì£¼
            except Exception:
                continue

            if art_dt < since_dt:
                stop_paging = True
                continue

            collected.append({
                "CID": cid,
                "TITLE": title.replace("<b>", "").replace("</b>", ""),
                "DATETIME": art_dt.strftime("%Y-%m-%d %H:%M:%S"),
                "URL": f"https://www.yna.co.kr/view/{cid}",
            })

        if stop_paging:
            break
        if len(results) < config.page_size:
            break

        time.sleep(0.15)

    return collected, last_raw


# =========================
# âœ… ë³¸ë¬¸/ì´ë¯¸ì§€ ì¶”ì¶œ(HTML)
# =========================
def try_extract_from_jsonld_text(soup: BeautifulSoup):
    scripts = soup.find_all("script", attrs={"type": "application/ld+json"})
    for sc in scripts:
        raw = sc.get_text(strip=True)
        if not raw:
            continue
        try:
            obj = json.loads(raw)
        except Exception:
            continue

        candidates = obj if isinstance(obj, list) else [obj]
        for it in candidates:
            if not isinstance(it, dict):
                continue
            body = it.get("articleBody") or it.get("description")
            if isinstance(body, str):
                body = clean_yna_content(normalize_text(body))
                if len(body) >= 80:
                    return body, "jsonld"
    return "", ""

def best_text_block_by_heuristic(soup: BeautifulSoup):
    best_text = ""
    best_src = ""
    best_score = 0

    for tag in soup.find_all(["article", "div", "section"], limit=4000):
        ident = " ".join(
            filter(
                None,
                [
                    tag.get("id", ""),
                    " ".join(tag.get("class", []) if tag.get("class") else []),
                ],
            )
        ).lower()

        if any(x in ident for x in ["nav", "menu", "footer", "header", "aside", "banner", "ad", "promo", "related", "share"]):
            continue

        text = normalize_text(tag.get_text("\n"))
        if len(text) < 250:
            continue

        p_cnt = len(tag.find_all("p"))
        score = len(text) + p_cnt * 120

        a_cnt = len(tag.find_all("a"))
        if a_cnt >= 25 and p_cnt == 0:
            continue

        if score > best_score:
            best_score = score
            best_text = text
            best_src = "heuristic"

    best_text = clean_yna_content(best_text)
    return best_text, best_src

def extract_yna_content(html: str):
    soup = BeautifulSoup(html, "html.parser")

    for t in soup(["script", "style", "noscript"]):
        t.decompose()

    body, src = try_extract_from_jsonld_text(soup)
    if body:
        return body, src

    selectors = [
        "#articleBodyContents",
        "#articleBody",
        "#articleWrap",
        ".story-news",
        ".article",
        ".content01",
        "article",
    ]

    best_text = ""
    best_src = ""
    for sel in selectors:
        node = soup.select_one(sel)
        if not node:
            continue
        text = clean_yna_content(normalize_text(node.get_text("\n")))
        if len(text) > len(best_text):
            best_text = text
            best_src = f"selector:{sel}"

    if best_text and len(best_text) >= 80:
        return best_text, best_src

    text2, src2 = best_text_block_by_heuristic(soup)
    if text2 and len(text2) >= 80:
        return text2, src2

    meta = soup.find("meta", attrs={"name": "description"})
    if meta and meta.get("content"):
        text3 = clean_yna_content(normalize_text(meta["content"]))
        if len(text3) >= 40:
            return text3, "meta:description"

    return "", "empty"

def extract_yna_images(html: str, article_url: str):
    soup = BeautifulSoup(html, "html.parser")
    urls = []
    source = "empty"

    m = soup.find("meta", attrs={"property": "og:image"})
    if m and m.get("content"):
        u = absolutize_img_url(m["content"], article_url)
        if u:
            urls.append(u)
            source = "meta:og:image"

    m = soup.find("meta", attrs={"name": "twitter:image"})
    if m and m.get("content"):
        u = absolutize_img_url(m["content"], article_url)
        if u:
            urls.append(u)
            if source == "empty":
                source = "meta:twitter:image"

    scripts = soup.find_all("script", attrs={"type": "application/ld+json"})
    for sc in scripts:
        raw = sc.get_text(strip=True)
        if not raw:
            continue
        try:
            obj = json.loads(raw)
        except Exception:
            continue
        candidates = obj if isinstance(obj, list) else [obj]
        for it in candidates:
            if not isinstance(it, dict):
                continue
            img = it.get("image")
            img_url = ""
            if isinstance(img, str):
                img_url = img
            elif isinstance(img, list) and img:
                img_url = img[0] if isinstance(img[0], str) else ""
            elif isinstance(img, dict):
                img_url = img.get("url") or img.get("@id") or ""
            img_url = absolutize_img_url(img_url, article_url)
            if img_url:
                urls.append(img_url)
                if source == "empty":
                    source = "jsonld:image"

    for img_tag in soup.select("figure.image-zone01 img"):
        cand = img_tag.get("src") or img_tag.get("data-src") or img_tag.get("data-original")
        cand = absolutize_img_url(cand, article_url)
        if cand:
            urls.append(cand)
            if source == "empty":
                source = "selector:figure.image-zone01"

    selectors = [
        "#articleBodyContents img",
        "#articleBody img",
        "#articleWrap img",
        ".story-news img",
        ".article img",
        ".content01 img",
        "article img",
    ]
    for sel in selectors:
        for img_tag in soup.select(sel):
            cand = img_tag.get("src") or img_tag.get("data-src") or img_tag.get("data-original")
            cand = absolutize_img_url(cand, article_url)
            if cand:
                urls.append(cand)
                if source == "empty":
                    source = f"selector:{sel}"

    urls = uniq_keep_order(urls)
    primary = urls[0] if urls else ""
    return primary, urls, source

def fetch_article_page(session: requests.Session, config: CrawlConfig, url: str):
    last_err = ""
    for attempt in range(config.content_retries + 1):
        try:
            resp = session.get(url, headers=headers_for_html(), timeout=config.content_timeout)
            status = resp.status_code

            if status in (403, 429, 500, 502, 503, 504) and attempt < config.content_retries:
                time.sleep(config.content_retry_backoff ** (attempt + 1))
                continue

            resp.raise_for_status()

            if resp.encoding is None or resp.encoding.lower() == "iso-8859-1":
                resp.encoding = resp.apparent_encoding

            html = resp.text

            content, content_source = extract_yna_content(html)
            if config.content_max_chars and len(content) > config.content_max_chars:
                content = content[: config.content_max_chars].rstrip() + "\n...(truncated)"

            image_url, image_urls, image_source = ("", [], "disabled")
            if config.fetch_image:
                image_url, image_urls, image_source = extract_yna_images(html, url)

            if not content or len(content.strip()) < 80:
                return {
                    "content": "",
                    "content_len": 0,
                    "content_status": f"EMPTY_HTTP_{status}",
                    "content_source": content_source,
                    "image_url": image_url,
                    "image_urls": image_urls,
                    "image_source": image_source,
                }

            return {
                "content": content,
                "content_len": len(content),
                "content_status": "OK",
                "content_source": content_source,
                "image_url": image_url,
                "image_urls": image_urls,
                "image_source": image_source,
            }

        except Exception as e:
            last_err = str(e)
            if attempt < config.content_retries:
                time.sleep(config.content_retry_backoff ** (attempt + 1))
                continue

    return {
        "content": "",
        "content_len": 0,
        "content_status": f"ERROR:{last_err[:120]}",
        "content_source": "error",
        "image_url": "",
        "image_urls": [],
        "image_source": "error",
    }


def enrich_rows_with_content_and_image(session: requests.Session, config: CrawlConfig, flat_rows: List[dict]):
    if not flat_rows:
        return
    print("\n[DETAIL] ë³¸ë¬¸/ì´ë¯¸ì§€ í¬ë¡¤ë§ ì‹œì‘...")
    for i, row in enumerate(flat_rows, 1):
        url = row.get("URL")
        if not url:
            row["CONTENT"] = ""
            row["CONTENT_LEN"] = 0
            row["CONTENT_STATUS"] = "NO_URL"
            row["CONTENT_SOURCE"] = "none"
            row["IMAGE_URL"] = ""
            row["IMAGE_URLS"] = "[]"
            row["IMAGE_SOURCE"] = "none"
            row["SENTIMENT_LABEL"] = "neutral"
            row["SENTIMENT_SCORE"] = 0.0
            continue

        r = fetch_article_page(session, config, url)

        row["CONTENT"] = r["content"]
        row["CONTENT_LEN"] = r["content_len"]
        row["CONTENT_STATUS"] = r["content_status"]
        row["CONTENT_SOURCE"] = r["content_source"]

        row["IMAGE_URL"] = r.get("image_url", "")
        row["IMAGE_URLS"] = json.dumps(r.get("image_urls", []), ensure_ascii=False)
        row["IMAGE_SOURCE"] = r.get("image_source", "")

        # âœ… ê°ì„± ë¶„ì„ ìˆ˜í–‰
        s_label, s_score = "neutral", 0.0
        if config.sentiment_enabled and sentiment_service:
            text_for_sent = (r["content"].strip() or row.get("TITLE", "").strip())
            if text_for_sent:
                try:
                    s_label, s_score = sentiment_service.predict(text_for_sent)
                except Exception:
                    pass
        row["SENTIMENT_LABEL"] = s_label
        row["SENTIMENT_SCORE"] = float(s_score)

        if i % 10 == 0 or i == len(flat_rows):
            ok_cnt = sum(1 for rr in flat_rows if rr.get("CONTENT_STATUS") == "OK")
            img_cnt = sum(1 for rr in flat_rows if rr.get("IMAGE_URL"))
            print(f"  - ì§„í–‰ {i}/{len(flat_rows)} | CONTENT_OK {ok_cnt} | IMAGE(primary) {img_cnt}")

        time.sleep(config.content_sleep_sec)

    # âœ… ê°ì„± í•„í„° ì ìš© (ìˆì„ ê²½ìš°)
    if config.sentiment_filter:
        before_cnt = len(flat_rows)
        flat_rows[:] = [r for r in flat_rows if r.get("SENTIMENT_LABEL") in config.sentiment_filter]
        after_cnt = len(flat_rows)
        if before_cnt != after_cnt:
            print(f"ğŸ” ê°ì„± í•„í„° ì ìš©: {before_cnt}ê±´ -> {after_cnt}ê±´ (í•„í„°: {config.sentiment_filter})")

    print("[DETAIL] ë³¸ë¬¸/ì´ë¯¸ì§€ í¬ë¡¤ë§ ì™„ë£Œ.\n")


# =========================
# ì €ì¥(CSV)
# =========================
def make_output_filename(config: CrawlConfig, since_dt: datetime) -> str:
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    since_tag = since_dt.strftime("%Y%m%d")
    today_tag = datetime.now(KST).strftime("%Y%m%d")
    return f"{config.output_prefix}_{ts}_{since_tag}_to_{today_tag}_company_plus_person_only.csv"

def save_results_csv(flat_rows: List[dict], out_path: str):
    fieldnames = [
        "GROUP",
        "COMPANY_KEYWORD",
        "PERSON",
        "QUERY_USED",
        "CID",
        "DATETIME",
        "TITLE",
        "URL",
        "CONTENT",
        "CONTENT_LEN",
        "CONTENT_STATUS",
        "CONTENT_SOURCE",
        "IMAGE_URL",
        "IMAGE_URLS",
        "IMAGE_SOURCE",
        "SENTIMENT_LABEL",
        "SENTIMENT_SCORE",
    ]
    with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        w.writeheader()
        w.writerows(flat_rows)


# =========================
# âœ… MongoDB doc ë³€í™˜
# =========================
def parse_dt_any(val: Any) -> Optional[datetime]:
    if isinstance(val, datetime):
        return val
    if isinstance(val, str):
        s = val.strip()
        try:
            return datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
        except Exception:
            return None
    return None

def build_mongo_doc_from_row(row: dict) -> dict:
    dt_val = (row.get("DATETIME", "") or "").strip()
    dt_obj = None
    try:
        if dt_val:
            dt_obj = datetime.strptime(dt_val, "%Y-%m-%d %H:%M:%S")
    except Exception:
        dt_obj = None

    title = row.get("TITLE", "") or ""
    content = row.get("CONTENT", "") or ""
    text_for_sent = (content.strip() or title.strip())
    # ê¸°ë³¸ê°’ì€ "" ë˜ëŠ” Noneìœ¼ë¡œ ë‘ì–´ ë°±í•„ ëŒ€ìƒì´ ë  ìˆ˜ ìˆê²Œ í•¨ (ì´ë¯¸ ë¶„ì„ëœ ê²½ìš°ëŠ” ì œì™¸)
    s_label = row.get("SENTIMENT_LABEL", "")
    s_score = row.get("SENTIMENT_SCORE", None)

    # rowì— ì´ë¯¸ ìˆìœ¼ë©´ (í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œ ì±„ì›Œì§) ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ í•œ ë²ˆ ë” ì‹œë„ (ì˜µì…˜)
    if not s_label and sentiment_service is not None and text_for_sent:
        try:
            s_label, s_score = sentiment_service.predict(text_for_sent)
        except Exception:
            s_label, s_score = "neutral", 0.0

    return {
        "Group": row.get("GROUP", ""),
        "PERSON": row.get("PERSON", ""),
        "CID": (row.get("CID", "") or "").strip(),
        "DATETIME": dt_obj if dt_obj else dt_val,
        "TITLE": title,
        "CONTENT": content,
        "URL": row.get("URL", ""),
        "IMAGE_URL": row.get("IMAGE_URL", ""),
        "IMAGE_URLS": json.loads(row.get("IMAGE_URLS", "[]")) if row.get("IMAGE_URLS") else [],
        "sentiment_label": s_label,
        "sentiment_score": float(s_score),
    }


# =========================
# âœ… (ì‚½ì… ì „) ì¤‘ë³µ ì œê±° ë¡œì§ (ê¸°ì—… GROUP ë‹¨ìœ„)
# =========================
def load_existing_cids_for_group(col, group: str, limit: int = 50000) -> set:
    """
    ê·¸ë£¹ë³„ë¡œ ìµœê·¼ ë¬¸ì„œì—ì„œ CIDë¥¼ ë¡œë“œ.
    ì—„ì²­ í° ì»¬ë ‰ì…˜ì´ë©´ limit ì¡°ì ˆ.
    """
    cids = set()
    cur = col.find({"Group": group, "CID": {"$exists": True, "$ne": ""}}, {"CID": 1}).sort("_id", -1).limit(limit)
    for d in cur:
        cid = (d.get("CID") or "").strip()
        if cid:
            cids.add(cid)
    return cids

def load_existing_titles_for_group(col, group: str, cfg: CrawlConfig) -> List[Tuple[str, str]]:
    """
    ê·¸ë£¹ë³„ ìµœê·¼ Nê°œ ì œëª© ë¡œë“œ -> (title_key, original_title)
    """
    out: List[Tuple[str, str]] = []
    cur = col.find({"Group": group, "TITLE": {"$exists": True, "$ne": ""}}, {"TITLE": 1, "DATETIME": 1}).sort("_id", -1).limit(cfg.title_dedup_db_limit_per_group)
    cutoff = datetime.now(KST).replace(tzinfo=None) - timedelta(days=cfg.title_dedup_only_within_days)
    for d in cur:
        t = (d.get("TITLE") or "").strip()
        if not t:
            continue
        dtv = parse_dt_any(d.get("DATETIME"))
        # ì˜¤ë˜ëœ ê¸°ì‚¬ëŠ” ë¹„êµ ëŒ€ìƒì—ì„œ ì œì™¸(ì˜¤íƒ ì¤„ì´ê¸°)
        if dtv and dtv < cutoff:
            continue
        out.append((title_key(t), t))
    return out

def dedup_rows_before_insert(flat_rows: List[dict], cfg: CrawlConfig, col=None) -> Tuple[List[dict], dict]:
    """
    ë°˜í™˜:
      - filtered_rows
      - stats
    """
    stats = {
        "input": len(flat_rows),
        "cid_removed_batch": 0,
        "cid_skipped_db": 0,
        "title_removed_batch": 0,
        "title_skipped_db": 0,
        "kept": 0,
    }

    if not cfg.pre_insert_dedup_enable or not flat_rows:
        stats["kept"] = len(flat_rows)
        return flat_rows, stats

    # ê·¸ë£¹ë³„ë¡œ ì²˜ë¦¬
    by_group: Dict[str, List[dict]] = {}
    for r in flat_rows:
        g = (r.get("GROUP") or "").strip()
        by_group.setdefault(g, []).append(r)

    filtered_all: List[dict] = []

    for g, rows in by_group.items():
        # DATETIME ìµœì‹  ìš°ì„ (ìœ ì‚¬ë„ dedupì—ì„œ ìµœì‹ ì„ ë‚¨ê¸°ê¸°)
        def _dtv(rr):
            try:
                return datetime.strptime((rr.get("DATETIME") or ""), "%Y-%m-%d %H:%M:%S")
            except Exception:
                return datetime.min
        rows_sorted = sorted(rows, key=_dtv, reverse=True)

        # 1) CID ë°°ì¹˜ ë‚´ë¶€ ì¤‘ë³µ ì œê±°
        seen_cid = set()
        rows_cid_unique: List[dict] = []
        for r in rows_sorted:
            cid = (r.get("CID") or "").strip()
            if not cid:
                continue
            if cid in seen_cid:
                stats["cid_removed_batch"] += 1
                continue
            seen_cid.add(cid)
            rows_cid_unique.append(r)

        # 1-2) DBì— CIDê°€ ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ(ì„ íƒ)
        existing_cids = set()
        if col is not None and cfg.pre_insert_skip_if_cid_exists_in_db:
            # ê·¸ë£¹ë³„ CID set(ìµœê·¼ ê¸°ì¤€)
            existing_cids = load_existing_cids_for_group(col, g, limit=50000)

        rows_after_cid_db: List[dict] = []
        for r in rows_cid_unique:
            cid = (r.get("CID") or "").strip()
            if existing_cids and cid in existing_cids:
                stats["cid_skipped_db"] += 1
                continue
            rows_after_cid_db.append(r)

        # 2) ì œëª© ìœ ì‚¬/ì¤‘ë³µ ì œê±°
        if not cfg.title_dedup_enable:
            filtered_all.extend(rows_after_cid_db)
            continue

        # DB ìµœê·¼ ì œëª© ë¡œë“œ(ì„ íƒ)
        existing_title_keys: List[str] = []
        existing_titles_orig: List[str] = []
        if col is not None and cfg.title_dedup_db_lookup:
            ex = load_existing_titles_for_group(col, g, cfg)
            existing_title_keys = [k for k, _ in ex if k]
            existing_titles_orig = [t for _, t in ex]

        kept: List[dict] = []
        kept_title_keys: List[str] = []
        kept_titles_orig: List[str] = []

        for r in rows_after_cid_db:
            t = (r.get("TITLE") or "").strip()
            tk = title_key(t)
            if not tk:
                kept.append(r)
                continue

            # (A) ë°°ì¹˜ ë‚´ ì¤‘ë³µ ê²€ì‚¬(ìµœê·¼ kept Nê°œë§Œ ë¹„êµ)
            dup_in_batch = False
            start_idx = max(0, len(kept_title_keys) - cfg.title_dedup_compare_recent_keep)
            for k2, t2 in zip(kept_title_keys[start_idx:], kept_titles_orig[start_idx:]):
                if not k2:
                    continue
                # key ë™ì¼ì´ë©´ ì¦‰ì‹œ duplicate
                if tk == k2:
                    dup_in_batch = True
                    break
                # ìœ ì‚¬ë„ ê²€ì‚¬
                if is_title_duplicate(t, t2, cfg):
                    dup_in_batch = True
                    break
            if dup_in_batch:
                stats["title_removed_batch"] += 1
                continue

            # (B) DB ê¸°ì¡´ ê¸°ì‚¬ì™€ ì¤‘ë³µ ê²€ì‚¬(ì„ íƒ)
            dup_in_db = False
            if existing_title_keys:
                # key ë™ì¼ì´ë©´ ë¹ ë¥´ê²Œ ì¤‘ë³µ ì²˜ë¦¬
                if tk in existing_title_keys:
                    dup_in_db = True
                else:
                    # ë„ˆë¬´ ë§ì´ ë¹„êµí•˜ë©´ ëŠë ¤ì„œ, ìµœê·¼ Nê°œ ì›ë¬¸ íƒ€ì´í‹€ë§Œ ëŒ€ëµ ë¹„êµ
                    # (N=3000ì´ë©´ difflibê°€ ëŠë¦´ ìˆ˜ ìˆì–´ "key ê¸°ë°˜" ë¨¼ì €, ê·¸ë‹¤ìŒ ì œí•œ ë¹„êµ)
                    limit_compare = min(len(existing_titles_orig), 400)
                    for t2 in existing_titles_orig[:limit_compare]:
                        if is_title_duplicate(t, t2, cfg):
                            dup_in_db = True
                            break

            if dup_in_db:
                stats["title_skipped_db"] += 1
                continue

            kept.append(r)
            kept_title_keys.append(tk)
            kept_titles_orig.append(t)

        if cfg.title_dedup_verbose:
            print(f"[DEDUP] Group={g} | in={len(rows)} -> cid_unique={len(rows_cid_unique)} -> after_db_cid={len(rows_after_cid_db)} -> kept={len(kept)}")

        filtered_all.extend(kept)

    stats["kept"] = len(filtered_all)
    return filtered_all, stats


# =========================
# âœ… MongoDB ì—…ë¡œë“œ (ì‚½ì… ì „ dedup í¬í•¨)
# =========================
def upload_rows_to_mongo(flat_rows: List[dict], config: CrawlConfig):
    if not MongoClient or not UpdateOne:
        raise RuntimeError("pymongoê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pymongo")
    if not config.mongo_uri:
        raise RuntimeError("MONGO_URI is required. Set it in .env")

    client = MongoClient(config.mongo_uri)
    col = client[config.mongo_db][config.mongo_collection]

    # (ì„ íƒ) CID unique index
    if config.mongo_ensure_unique_cid:
        try:
            col.create_index([("CID", 1)], unique=True, background=True, name="uniq_CID")
            print("âœ… Mongo index ensured: CID unique")
        except Exception as e:
            print("âš ï¸ Mongo CID unique index ìƒì„± ì‹¤íŒ¨(ì´ë¯¸ ì¤‘ë³µì´ ìˆê±°ë‚˜ ê¶Œí•œ ë¬¸ì œ):", e)

    # âœ… ì‚½ì… ì „ ì¤‘ë³µ ì œê±°
    filtered_rows, st = dedup_rows_before_insert(flat_rows, config, col=col)
    print("âœ… PRE-INSERT DEDUP STATS:", st)

    ops = []
    sent = 0

    for row in filtered_rows:
        cid = (row.get("CID") or "").strip()
        if not cid:
            continue

        doc = build_mongo_doc_from_row(row)

        ops.append(
            UpdateOne(
                {"CID": cid},       # âœ… CID ê¸°ì¤€ upsert
                {"$set": doc},
                upsert=config.mongo_upsert,
            )
        )

        if len(ops) >= config.mongo_batch_size:
            result = col.bulk_write(ops, ordered=False)
            sent += len(ops)
            ops.clear()
            print("âœ… Mongo bulk ì—…ë¡œë“œ:", {
                "sent": sent,
                "matched": result.matched_count,
                "modified": result.modified_count,
                "upserted": len(result.upserted_ids or {}),
            })

    if ops:
        result = col.bulk_write(ops, ordered=False)
        sent += len(ops)
        ops.clear()
        print("âœ… Mongo bulk ì—…ë¡œë“œ:", {
            "sent": sent,
            "matched": result.matched_count,
            "modified": result.modified_count,
            "upserted": len(result.upserted_ids or {}),
        })

    try:
        client.close()
    except Exception:
        pass


# =========================
# âœ… (ì˜µì…˜) MongoDB ê¸°ì¡´ ì¤‘ë³µ ì •ë¦¬: GROUP ë‹¨ìœ„
# =========================
def cleanup_existing_duplicates(col, groups: List[str], cfg: CrawlConfig, apply: bool = False) -> dict:
    """
    1) CID ì¤‘ë³µ ì‚­ì œ
    2) ì œëª© ì¤‘ë³µ ì‚­ì œ(ì •ê·œí™” ë™ì¼ + ìœ ì‚¬ë„)
    ê¸°ë³¸ì€ DRY RUN (apply=False)
    """
    report = {
        "apply": apply,
        "groups": len(groups),
        "cid_dups_found": 0,
        "cid_docs_to_delete": 0,
        "title_dups_found": 0,
        "title_docs_to_delete": 0,
    }

    now_cutoff = datetime.now(KST).replace(tzinfo=None) - timedelta(days=max(cfg.title_dedup_only_within_days, 7))

    for g in groups:
        # ---------- 1) CID ì¤‘ë³µ ----------
        pipeline = [
            {"$match": {"Group": g, "CID": {"$exists": True, "$ne": ""}}},
            {"$group": {"_id": "$CID", "ids": {"$push": "$_id"}, "count": {"$sum": 1}}},
            {"$match": {"count": {"$gt": 1}}},
        ]
        dups = list(col.aggregate(pipeline, allowDiskUse=True))
        if dups:
            report["cid_dups_found"] += len(dups)

        for d in dups:
            ids = d["ids"]
            # ì–´ë–¤ ê±¸ ë‚¨ê¸¸ì§€ ê²°ì •: DATETIME ìµœì‹ , ì—†ìœ¼ë©´ _id ìµœì‹ 
            docs = list(col.find({"_id": {"$in": ids}}, {"DATETIME": 1, "TITLE": 1}))
            def _score(doc):
                dtv = parse_dt_any(doc.get("DATETIME"))
                # dt ì—†ìœ¼ë©´ _idì˜ ìƒì„±ì‹œê°
                oid = doc.get("_id")
                oid_time = oid.generation_time.replace(tzinfo=None) if hasattr(oid, "generation_time") else datetime.min
                return dtv if dtv else oid_time
            docs_sorted = sorted(docs, key=_score, reverse=True)
            keep_id = docs_sorted[0]["_id"]
            del_ids = [x["_id"] for x in docs_sorted[1:]]
            report["cid_docs_to_delete"] += len(del_ids)

            if apply and del_ids:
                col.delete_many({"_id": {"$in": del_ids}})

        # ---------- 2) ì œëª© ì¤‘ë³µ ----------
        # ìµœê·¼ ë¬¸ì„œ ìœ„ì£¼ë¡œë§Œ(ì˜¤íƒ/ì‹œê°„ ì´ìŠˆ ë°©ì§€)
        cur = col.find({"Group": g, "TITLE": {"$exists": True, "$ne": ""}}, {"TITLE": 1, "DATETIME": 1}).sort("_id", -1).limit(20000)
        kept_keys: List[str] = []
        kept_titles: List[str] = []
        kept_ids: List[Any] = []
        to_delete: List[Any] = []

        for doc in cur:
            dtv = parse_dt_any(doc.get("DATETIME"))
            if dtv and dtv < now_cutoff:
                # ë„ˆë¬´ ì˜¤ë˜ëœ ê±´ ë¹„êµ/ì •ë¦¬ ëŒ€ìƒì—ì„œ ì œì™¸
                continue

            tid = doc["_id"]
            t = (doc.get("TITLE") or "").strip()
            tk = title_key(t)
            if not tk:
                kept_ids.append(tid)
                kept_titles.append(t)
                kept_keys.append(tk)
                continue

            dup = False
            start_idx = max(0, len(kept_keys) - cfg.title_dedup_compare_recent_keep)
            for k2, t2 in zip(kept_keys[start_idx:], kept_titles[start_idx:]):
                if not k2:
                    continue
                if tk == k2:
                    dup = True
                    break
                if is_title_duplicate(t, t2, cfg):
                    dup = True
                    break

            if dup:
                to_delete.append(tid)
            else:
                kept_ids.append(tid)
                kept_titles.append(t)
                kept_keys.append(tk)

        if to_delete:
            report["title_dups_found"] += 1
            report["title_docs_to_delete"] += len(to_delete)
            if apply:
                col.delete_many({"_id": {"$in": to_delete}})

        print(f"[CLEANUP] Group={g} | cid_dups={len(dups)} | title_del={len(to_delete)} | apply={apply}")

    return report

# =========================
# âœ… (ì˜µì…˜) MongoDB ê¸°ì¡´ ê¸°ì‚¬ ê°ì„± ë¶„ì„ ì±„ìš°ê¸° (Backfill)
# =========================
def backfill_sentiment_in_mongo(config: CrawlConfig, limit: int = 1000, force: bool = False):
    if not MongoClient or not sentiment_service:
        print("âŒ MongoClient ë˜ëŠ” sentiment_serviceê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    client = MongoClient(config.mongo_uri)
    col = client[config.mongo_db][config.mongo_collection]

    # sentiment_labelì´ ì—†ê±°ë‚˜, ë¹„ì–´ìˆê±°ë‚˜, ë¶„ì„ë˜ì§€ ì•Šì€ ê¸°ë³¸ê°’(neutral + 0.0)ì¸ ê²½ìš° ì°¾ê¸°
    if force:
        # ê°•ì œ ëª¨ë“œ: ëª¨ë“  ê¸°ì‚¬ ëŒ€ìƒ (ìµœì‹ ìˆœ)
        query = {}
        print("âš ï¸ ê°•ì œ ì¬ë¶„ì„ ëª¨ë“œ (--force): ëª¨ë“  ê¸°ì¡´ ê¸°ì‚¬ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤.")
    else:
        query = {
            "$or": [
                {"sentiment_label": {"$exists": False}},
                {"sentiment_label": None},
                {"sentiment_label": ""},
                {"$and": [{"sentiment_label": "neutral"}, {"sentiment_score": 0.0}]} # ê¸°ë³¸ê°’ìœ¼ë¡œ ë“¤ì–´ê°„ ê²ƒë“¤ ì¬ë¶„ì„
            ]
        }

    total_to_process = col.count_documents(query)
    print(f"ğŸ” ê°ì„± ë¶„ì„ì´ í•„ìš”í•œ ê¸°ì¡´ ê¸°ì‚¬: {total_to_process}ê±´ (ì²˜ë¦¬ ì œí•œ: {limit}ê±´)")

    docs = list(col.find(query).sort("DATETIME", -1).limit(limit))
    if not docs:
        print("âœ… ì²˜ë¦¬í•  ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        client.close()
        return

    ops = []
    processed = 0
    for d in docs:
        text = (d.get("CONTENT") or d.get("TITLE") or "").strip()
        if not text:
            label, score = "neutral", 0.0
        else:
            try:
                label, score = sentiment_service.predict(text)
            except Exception:
                label, score = "neutral", 0.0

        ops.append(
            UpdateOne(
                {"_id": d["_id"]},
                {"$set": {"sentiment_label": label, "sentiment_score": float(score)}}
            )
        )
        processed += 1

        if len(ops) >= config.mongo_batch_size:
            col.bulk_write(ops, ordered=False)
            ops.clear()
            print(f"  - ì§„í–‰ ì¤‘... {processed}/{len(docs)}")

    if ops:
        col.bulk_write(ops, ordered=False)

    print(f"âœ… ë°±í•„ ì™„ë£Œ: {processed}ê±´ ì²˜ë¦¬ë¨")
    client.close()


# =========================
# search_groups ë¡œë“œ(ì˜µì…˜)
# =========================
def load_search_groups_from_csv(config: CrawlConfig) -> Dict[str, Tuple[str, str]]:
    groups: Dict[str, Tuple[str, str]] = {}
    with open(config.groups_csv_path, "r", encoding="utf-8-sig", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            g = (row.get(config.groups_csv_group_col) or "").strip()
            c = (row.get(config.groups_csv_company_col) or "").strip()
            p = (row.get(config.groups_csv_person_col) or "").strip()
            if not g:
                continue
            if not c or not p:
                continue
            groups[g] = (c, p)
    return groups


# =========================
# ë©”ì¸ í¬ë¡¤ëŸ¬
# =========================
def get_news(search_groups: Dict[str, Tuple[str, str]], config: CrawlConfig, since_dt: datetime):
    print(f"ì¡°íšŒ ê¸°ì¤€ ì‹œê°„: {since_dt.strftime('%Y-%m-%d %H:%M:%S')} ì´í›„ ê¸°ì‚¬")
    print(f"ì ìš© í•„í„°: cattr={config.cattr}, div_code={config.div_code}")
    print(f"í˜ì´ì§•: max_pages={config.max_pages} / page_size={config.page_size}")
    print(f"ë³¸ë¬¸(fetch_content)={config.fetch_content} / ì´ë¯¸ì§€(fetch_image)={config.fetch_image}")
    print(f"ê·¸ë£¹ ìˆ˜: {len(search_groups)}\n")

    session = requests.Session()
    session.mount("https://", LegacySSLAdapter())

    final_results = {}
    flat_rows = []

    for group_name, (company_kw, person_kw) in search_groups.items():
        group_articles = []
        seen_cids_group = set()

        queries = build_queries_company_plus_person(company_kw, person_kw)
        if not queries:
            print(f"âš ï¸ ê·¸ë£¹ '{group_name}' ìŠ¤í‚µ: company/person ëˆ„ë½ (company='{company_kw}', person='{person_kw}')")
            final_results[group_name] = []
            continue

        for q in queries:
            try:
                articles, _raw = fetch_articles_paged(session, config, q, since_dt)

                added = 0
                for a in articles:
                    cid = a["CID"]
                    if cid in seen_cids_group:
                        continue
                    seen_cids_group.add(cid)

                    a["KEYWORD_FOUND"] = q
                    group_articles.append(a)
                    added += 1

                    flat_rows.append(
                        {
                            "GROUP": group_name,
                            "COMPANY_KEYWORD": company_kw,
                            "PERSON": person_kw,
                            "QUERY_USED": q,
                            "CID": cid,
                            "DATETIME": a["DATETIME"],
                            "TITLE": a["TITLE"],
                            "URL": a["URL"],
                            "CONTENT": "",
                            "CONTENT_LEN": 0,
                            "CONTENT_STATUS": "PENDING",
                            "CONTENT_SOURCE": "",
                            "IMAGE_URL": "",
                            "IMAGE_URLS": "[]",
                            "IMAGE_SOURCE": "",
                            "SENTIMENT_LABEL": "neutral",
                            "SENTIMENT_SCORE": 0.0,
                        }
                    )

                print(f"âœ” ê·¸ë£¹ '{group_name}' / ì¿¼ë¦¬ '{q}' ì™„ë£Œ (ì¶”ê°€ {added}ê±´, ê·¸ë£¹ëˆ„ì  {len(group_articles)}ê±´)")
                time.sleep(config.sleep_sec)

            except Exception as e:
                print(f"âŒ ê·¸ë£¹ '{group_name}' / ì¿¼ë¦¬ '{q}' ì˜¤ë¥˜: {e}")

        final_results[group_name] = group_articles

    if config.fetch_content and flat_rows:
        enrich_rows_with_content_and_image(session, config, flat_rows)

    return final_results, flat_rows


# =========================
# ì‹¤í–‰ë¶€
# =========================
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--cleanup-existing", action="store_true", help="MongoDBì— ì´ë¯¸ ë“¤ì–´ê°„ ì¤‘ë³µ(CID/ì œëª©)ì„ ì •ë¦¬(ê¸°ë³¸ DRY RUN)")
    ap.add_argument("--apply", action="store_true", help="--cleanup-existing ì™€ ê°™ì´ ì‚¬ìš©: ì‹¤ì œ ì‚­ì œ ì ìš©")
    ap.add_argument("--backfill-sentiment", action="store_true", help="MongoDBì˜ ê¸°ì¡´ ê¸°ì‚¬ ì¤‘ ê°ì„± ë¶„ì„ì´ ì—†ëŠ” ê²ƒë“¤ì„ ì²˜ë¦¬")
    ap.add_argument("--backfill-limit", type=int, default=1000, help="ë°±í•„ ì‹œ í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜")
    ap.add_argument("--force", action="store_true", help="ê°•ì œë¡œ ëª¨ë“  ëŒ€ìƒ(ì´ë¯¸ ë¶„ì„ëœ ê²ƒ í¬í•¨) ì¬ë¶„ì„")
    args = ap.parse_args()

    # 1) search_groups ì¤€ë¹„
    # (ì›í•˜ë©´ CSV ë¡œë“œë¡œ ë°”ê¾¸ì„¸ìš”)
    search_groups = {
        "ìœ ì§„ê·¸ë£¹": ("ìœ ì§„ê·¸ë£¹", "ìœ ê²½ì„ "),
        "BGF": ("BGF", "í™ì„ì¡°"),
        "í˜„ëŒ€í•´ìƒ": ("í˜„ëŒ€í•´ìƒ", "ì •ëª½ìœ¤"),
        "í•˜ì´ë¸Œ": ("í•˜ì´ë¸Œ", "ë°©ì‹œí˜"),
        "í•œì†”": ("í•œì†”", "ì¡°ë™ê¸¸"),
        "ì‚¼ì„±": ("ì‚¼ì„±ì „ì", "ì´ì¬ìš©"),
        "SK": ("SK", "ìµœíƒœì›"),
        "í˜„ëŒ€ìë™ì°¨": ("í˜„ëŒ€ìë™ì°¨", "ì •ì˜ì„ "),
        "LG": ("LG", "êµ¬ê´‘ëª¨"),
        "ë¡¯ë°": ("ë¡¯ë°", "ì‹ ë™ë¹ˆ"),
        "í•œí™”": ("í•œí™”", "ê¹€ìŠ¹ì—°"),
        "HDí˜„ëŒ€": ("HDí˜„ëŒ€", "ì •ëª½ì¤€"),
        "GS": ("GS", "í—ˆì°½ìˆ˜"),
        "ì‹ ì„¸ê³„": ("ì‹ ì„¸ê³„", "ì´ëª…í¬"),
        "í•œì§„": ("í•œì§„", "ì¡°ì›íƒœ"),
        "CJ": ("CJ", "ì´ì¬í˜„"),
        "LS": ("LS", "êµ¬ìì€"),
        "ì¹´ì¹´ì˜¤": ("ì¹´ì¹´ì˜¤", "ê¹€ë²”ìˆ˜"),
        "ë‘ì‚°": ("ë‘ì‚°", "ë°•ì •ì›"),
        "DL": ("DL", "ì´í•´ìš±"),
        "ì¤‘í¥ê±´ì„¤": ("ì¤‘í¥ê±´ì„¤", "ì •ì°½ì„ "),
        "ì…€íŠ¸ë¦¬ì˜¨": ("ì…€íŠ¸ë¦¬ì˜¨", "ì„œì •ì§„"),
        "ë„¤ì´ë²„": ("ë„¤ì´ë²„", "ì´í•´ì§„"),
        "í˜„ëŒ€ë°±í™”ì ": ("í˜„ëŒ€ë°±í™”ì ", "ì •ì§€ì„ "),
        "í•œêµ­ì•¤ì»´í¼ë‹ˆê·¸ë£¹": ("í•œêµ­ì•¤ì»´í¼ë‹ˆê·¸ë£¹", "ì¡°ì–‘ë˜"),
        "ë¶€ì˜": ("ë¶€ì˜", "ì´ì¤‘ê·¼"),
        "í•˜ë¦¼": ("í•˜ë¦¼", "ê¹€í™êµ­"),
        "íš¨ì„±": ("íš¨ì„±", "ì¡°í˜„ì¤€"),
        "SM": ("SM", "ìš°ì˜¤í˜„"),
        "HDC": ("HDC", "ì •ëª½ê·œ"),
        "í˜¸ë°˜ê±´ì„¤": ("í˜¸ë°˜ê±´ì„¤", "ê¹€ìƒì—´"),
        "ì½”ì˜¤ë¡±": ("ì½”ì˜¤ë¡±", "ì´ì›…ì—´"),
        "KCC": ("KCC", "ì •ëª½ì§„"),
        "DB": ("DB", "ê¹€ì¤€ê¸°"),
        "OCI": ("OCI", "ì´ìš°í˜„"),
        "LX": ("LX", "êµ¬ë³¸ì¤€"),
        "ë„·ë§ˆë¸”": ("ë„·ë§ˆë¸”", "ë°©ì¤€í˜"),
        "ì´ëœë“œ": ("ì´ëœë“œ", "ë°•ì„±ìˆ˜"),
        "êµë³´ìƒëª…ë³´í—˜": ("êµë³´ìƒëª…ë³´í—˜", "ì‹ ì°½ì¬"),
        "ë‹¤ìš°í‚¤ì›€": ("ë‹¤ìš°í‚¤ì›€", "ê¹€ìµë˜"),
        "ê¸ˆí˜¸ì„ìœ í™”í•™": ("ê¸ˆí˜¸ì„ìœ í™”í•™", "ë°•ì°¬êµ¬"),
        "íƒœì˜": ("íƒœì˜", "ìœ¤ì„¸ì˜"),
        "KG": ("KG", "ê³½ì¬ì„ "),
        "HL": ("HL", "ì •ëª½ì›"),
        "ë™ì›": ("ë™ì›", "ê¹€ë‚¨ì •"),
        "ì•„ëª¨ë ˆí¼ì‹œí”½": ("ì•„ëª¨ë ˆí¼ì‹œí”½", "ì„œê²½ë°°"),
        "íƒœê´‘": ("íƒœê´‘", "ì´í˜¸ì§„"),
        "í¬ë˜í”„í†¤": ("í¬ë˜í”„í†¤", "ì¥ë³‘ê·œ"),
        "ì• ê²½": ("ì• ê²½", "ì¥ì˜ì‹ "),
        "ë™êµ­ì œê°•": ("ë™êµ­ì œê°•", "ì¥ì„¸ì£¼"),
        "ì¤‘ì•™": ("ì¤‘ì•™", "í™ì„í˜„"),
    }

    # (ì˜µì…˜) ê¸°ì¡´ Mongo ì¤‘ë³µ ì •ë¦¬
    if args.cleanup_existing:
        if not MongoClient or not CONFIG.mongo_uri:
            print("âŒ MongoClient ë˜ëŠ” MONGO_URIê°€ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        client = MongoClient(CONFIG.mongo_uri)
        col = client[CONFIG.mongo_db][CONFIG.mongo_collection]
        rep = cleanup_existing_duplicates(col, list(search_groups.keys()), CONFIG, apply=args.apply)
        print("âœ… CLEANUP REPORT:", rep)
        client.close()

    # 1-2) ê¸°ì¡´ ê¸°ì‚¬ ê°ì„± ë¶„ì„ ë°±í•„
    if args.backfill_sentiment:
        backfill_sentiment_in_mongo(CONFIG, limit=args.backfill_limit, force=args.force)
        # ë°±í•„ë§Œ í•˜ê³  ëë‚´ë ¤ë©´ ì—¬ê¸°ì„œ return
        # return


    # âœ… 2) ë‚ ì§œ ìë™ ê³„ì‚°
    since_dt = compute_since_dt_auto(CONFIG)
    if since_dt is None:
        sys.exit(0)

    # 3) ìˆ˜ì§‘
    _news_data, flat_rows = get_news(search_groups, CONFIG, since_dt)

    # 4) CSV ì €ì¥(ì›í•˜ë©´ ë„ê±°ë‚˜ ê²½ë¡œ ë³€ê²½)
    out_csv = make_output_filename(CONFIG, since_dt)
    save_results_csv(flat_rows, out_csv)
    print(f"\nâœ… CSV ì €ì¥ ì™„ë£Œ: {out_csv}")

    # 5) MongoDB ì—…ë¡œë“œ(ì‚½ì… ì „ CID+ì œëª© dedup ìˆ˜í–‰)
    if CONFIG.upload_to_mongo:
        upload_rows_to_mongo(flat_rows, CONFIG)

    # âœ… 6) state ì €ì¥: "ì˜¤ëŠ˜ ë‚ ì§œ"
    save_last_crawled_date(CONFIG.state_path, datetime.now(KST).date())
    print(f"âœ… state ì €ì¥ ì™„ë£Œ: {CONFIG.state_path}")

if __name__ == "__main__":
    main()
